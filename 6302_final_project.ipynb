{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyON5cjSD/ps+J6GJcIKl8Qo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prithwi13/6302_stock/blob/main/6302_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "first cell imports all the necessary libraries for your project. Most importantly, it securely loads your API keys from the Colab \"Secrets\" (ðŸ”‘) tab."
      ],
      "metadata": {
        "id": "Lu2t6ryL6P2R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aSiyJcwZgf6",
        "outputId": "555bea47-f2a0-435a-8c31-a40d776efc7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ API keys loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Optional\n",
        "import time\n",
        "import sys\n",
        "from google.colab import userdata\n",
        "\n",
        "# === SECURELY Load API keys from Colab Secrets ===\n",
        "try:\n",
        "    ALPHAVANTAGE_KEY = userdata.get('ALPHAVANTAGE_KEY')\n",
        "    NEWSAPI_KEY = userdata.get('NEWSAPI_KEY')\n",
        "    print(\"âœ“ API keys loaded successfully.\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"âœ— FATAL ERROR: API Key not found in Colab Secrets.\")\n",
        "    print(\"  Please click the 'Key' icon (ðŸ”‘) on the left sidebar,\")\n",
        "    print(\"  add 'ALPHAVANTAGE_KEY' and 'NEWSAPI_KEY',\")\n",
        "    print(\"  and make sure 'Notebook access' is toggled ON for both.\")\n",
        "    sys.exit(1) # Stop execution\n",
        "except Exception as e:\n",
        "    print(f\"âœ— FATAL ERROR: Could not load secrets. {e}\")\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code runs the import statements and then immediately tries to fetch your keys from the Colab Secrets manager. If it fails, it prints a helpful error message and stops the script."
      ],
      "metadata": {
        "id": "T8PlcLh240-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we define the classes responsible for fetching data from the two external APIs. These are your \"worker\" classes."
      ],
      "metadata": {
        "id": "wHbYZuPh6XEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StockPriceAPI:\n",
        "    \"\"\"Handles stock price data using Alpha Vantage\"\"\"\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        self.base_url = \"https://www.alphavantage.co/query\"\n",
        "        print(f\"âœ“ Alpha Vantage initialized (API key: ...{api_key[-4:]})\")\n",
        "\n",
        "    def get_intraday_quotes(self, ticker: str, interval='5m', period='7d') -> pd.DataFrame:\n",
        "        print(f\"  ... [Alpha Vantage] Waiting 13s to respect 5 calls/min rate limit...\")\n",
        "        time.sleep(13)\n",
        "        av_interval = interval.replace('m', 'min')\n",
        "        av_outputsize = 'compact' if period == '1d' else 'full'\n",
        "        params = {\n",
        "            'function': 'TIME_SERIES_INTRADAY', 'symbol': ticker, 'interval': av_interval,\n",
        "            'outputsize': av_outputsize, 'apikey': self.api_key, 'datatype': 'json'\n",
        "        }\n",
        "        try:\n",
        "            response = requests.get(self.base_url, params=params, timeout=20)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "            if 'Error Message' in data:\n",
        "                print(f\"âœ— [Alpha Vantage] API Error for {ticker}: {data['Error Message']}\")\n",
        "                return pd.DataFrame()\n",
        "            if 'Note' in data:\n",
        "                print(f\"âœ— [Alpha Vantage] API Note for {ticker}: {data['Note']}\")\n",
        "                return pd.DataFrame()\n",
        "            data_key = next((key for key in data.keys() if 'Time Series' in key), None)\n",
        "            if data_key is None:\n",
        "                print(f\"âœ— [Alpha Vantage] Could not find 'Time Series' data key for {ticker}.\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            df = pd.DataFrame.from_dict(data[data_key], orient='index')\n",
        "            if df.empty: return pd.DataFrame()\n",
        "\n",
        "            df = df.reset_index().rename(columns={\n",
        "                'index': 'timestamp', '1. open': 'open', '2. high': 'high',\n",
        "                '3. low': 'low', '4. close': 'close', '5. volume': 'volume'\n",
        "            })\n",
        "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "            for col in ['open', 'high', 'low', 'close', 'volume']:\n",
        "                df[col] = pd.to_numeric(df[col])\n",
        "            df['ticker'] = ticker\n",
        "            try:\n",
        "                df['timestamp'] = df['timestamp'].dt.tz_localize('America/New_York').dt.tz_convert('UTC')\n",
        "            except Exception:\n",
        "                df['timestamp'] = df['timestamp'].dt.tz_localize('UTC') # Fallback\n",
        "            df = df[['timestamp', 'ticker', 'open', 'high', 'low', 'close', 'volume']]\n",
        "            return df.sort_values('timestamp').reset_index(drop=True)\n",
        "        except Exception as e:\n",
        "            print(f\"âœ— Error fetching Alpha Vantage data for {ticker}: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "class NewsAPI:\n",
        "    \"\"\"Handles financial news from NewsAPI\"\"\"\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        print(f\"âœ“ NewsAPI initialized (API key: ...{api_key[-4:]})\")\n",
        "\n",
        "    def get_news(self, ticker: str, company_name: str = None, days_back: int = 7) -> pd.DataFrame:\n",
        "        url = 'https://newsapi.org/v2/everything'\n",
        "        query = company_name if company_name else ticker\n",
        "        from_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
        "        params = {\n",
        "            'q': query, 'from': from_date, 'sortBy': 'publishedAt',\n",
        "            'language': 'en', 'apiKey': self.api_key, 'pageSize': 100\n",
        "        }\n",
        "        try:\n",
        "            response = requests.get(url, params=params, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "            if data['status'] != 'ok':\n",
        "                raise ValueError(f\"API Error: {data.get('message', 'Unknown error')}\")\n",
        "\n",
        "            records = [{\n",
        "                'timestamp': pd.to_datetime(article['publishedAt']).tz_convert('UTC'),\n",
        "                'headline': article['title'],\n",
        "                'description': article.get('description', ''),\n",
        "                'source': article['source']['name'],\n",
        "                'url': article['url'],\n",
        "                'ticker': ticker\n",
        "            } for article in data.get('articles', []) if article.get('title')]\n",
        "\n",
        "            return pd.DataFrame(records).sort_values('timestamp').reset_index(drop=True) if records else pd.DataFrame()\n",
        "        except Exception as e:\n",
        "            print(f\"âœ— Error fetching news for {ticker}: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "print(\"âœ“ Data fetching classes defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp6mV0gQ41ex",
        "outputId": "35972662-6348-4a2d-f299-4482b4465c01"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Data fetching classes defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the classes. It doesn't run them yet, it just makes them available for later use. Note the critical time.sleep(13) in StockPriceAPIâ€”this is essential for not getting blocked by the free API."
      ],
      "metadata": {
        "id": "x8i0_m1O48KH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These classes are the \"brains\" of the operation. IncrementalDataStorage is the most important partâ€”it handles saving data and preventing duplicates. DataCollector manages the entire process."
      ],
      "metadata": {
        "id": "4y3haFEu48hD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IncrementalDataStorage:\n",
        "    \"\"\"Handles incremental data storage and deduplication.\"\"\"\n",
        "    def __init__(self, data_dir='data/raw'):\n",
        "        self.data_dir = data_dir\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "        self.prices_master_file = os.path.join(data_dir, 'prices_master.csv')\n",
        "        self.news_master_file = os.path.join(data_dir, 'news_master.csv')\n",
        "        self.backup_dir = os.path.join(data_dir, 'daily_backups')\n",
        "        os.makedirs(self.backup_dir, exist_ok=True)\n",
        "\n",
        "    def load_master_data(self) -> Dict[str, pd.DataFrame]:\n",
        "        prices = pd.DataFrame()\n",
        "        news = pd.DataFrame()\n",
        "        if os.path.exists(self.prices_master_file):\n",
        "            prices = pd.read_csv(self.prices_master_file)\n",
        "            prices['timestamp'] = pd.to_datetime(prices['timestamp'])\n",
        "            print(f\"âœ“ Loaded {len(prices):,} existing price records\")\n",
        "        else:\n",
        "            print(\"â„¹ No existing price data found (starting fresh)\")\n",
        "        if os.path.exists(self.news_master_file):\n",
        "            news = pd.read_csv(self.news_master_file)\n",
        "            news['timestamp'] = pd.to_datetime(news['timestamp'])\n",
        "            print(f\"âœ“ Loaded {len(news):,} existing news records\")\n",
        "        else:\n",
        "            print(\"â„¹ No existing news data found (starting fresh)\")\n",
        "        return {'prices': prices, 'news': news}\n",
        "\n",
        "    def append_new_data(self, new_data: Dict[str, pd.DataFrame]):\n",
        "        existing = self.load_master_data()\n",
        "        if not new_data['prices'].empty:\n",
        "            combined_prices = pd.concat([existing['prices'], new_data['prices']], ignore_index=True)\n",
        "            combined_prices = combined_prices.drop_duplicates(subset=['timestamp', 'ticker'], keep='last')\n",
        "            combined_prices = combined_prices.sort_values(['ticker', 'timestamp']).reset_index(drop=True)\n",
        "            combined_prices.to_csv(self.prices_master_file, index=False)\n",
        "            print(f\"âœ“ Saved {len(combined_prices):,} total price records (added {len(new_data['prices'])} new)\")\n",
        "        if not new_data['news'].empty:\n",
        "            combined_news = pd.concat([existing['news'], new_data['news']], ignore_index=True)\n",
        "            combined_news = combined_news.drop_duplicates(subset=['headline', 'ticker'], keep='first')\n",
        "            combined_news = combined_news.sort_values(['ticker', 'timestamp']).reset_index(drop=True)\n",
        "            combined_news.to_csv(self.news_master_file, index=False)\n",
        "            print(f\"âœ“ Saved {len(combined_news):,} total news records (added {len(new_data['news'])} new)\")\n",
        "\n",
        "    def get_statistics(self) -> dict:\n",
        "        data = self.load_master_data()\n",
        "        stats = {'total_price_records': len(data['prices']), 'total_news_articles': len(data['news']), 'tickers': [], 'date_range': None}\n",
        "        if not data['prices'].empty:\n",
        "            stats['tickers'] = data['prices']['ticker'].unique().tolist()\n",
        "            stats['date_range'] = {'start': data['prices']['timestamp'].min(), 'end': data['prices']['timestamp'].max()}\n",
        "            stats['records_per_ticker'] = data['prices'].groupby('ticker').size().to_dict()\n",
        "        return stats\n",
        "\n",
        "class DataCollector:\n",
        "    \"\"\"Main data collection orchestrator\"\"\"\n",
        "    def __init__(self, price_api: StockPriceAPI, news_api: NewsAPI, storage: IncrementalDataStorage):\n",
        "        self.price_api = price_api\n",
        "        self.news_api = news_api\n",
        "        self.storage = storage\n",
        "\n",
        "    def collect_and_store(self, tickers: List[str], interval='5m', period='1d', news_days_back=1):\n",
        "        all_prices, all_news = [], []\n",
        "        company_map = {'AAPL': 'Apple', 'MSFT': 'Microsoft', 'GOOGL': 'Google', 'AMZN': 'Amazon', 'TSLA': 'Tesla'}\n",
        "        for ticker in tickers:\n",
        "            print(f\"\\nðŸ“Š Collecting data for {ticker}...\")\n",
        "            prices = self.price_api.get_intraday_quotes(ticker, interval, period)\n",
        "            if not prices.empty:\n",
        "                all_prices.append(prices)\n",
        "                print(f\"  âœ“ {len(prices)} price records\")\n",
        "            company_name = company_map.get(ticker, ticker)\n",
        "            news = self.news_api.get_news(ticker, company_name, news_days_back)\n",
        "            if not news.empty:\n",
        "                all_news.append(news)\n",
        "                print(f\"  âœ“ {len(news)} news articles\")\n",
        "\n",
        "        new_data = {\n",
        "            'prices': pd.concat(all_prices, ignore_index=True) if all_prices else pd.DataFrame(),\n",
        "            'news': pd.concat(all_news, ignore_index=True) if all_news else pd.DataFrame()\n",
        "        }\n",
        "        self.storage.append_new_data(new_data)\n",
        "        return new_data\n",
        "\n",
        "print(\"âœ“ Data storage and collector classes defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrRoAQAA487D",
        "outputId": "12f5e0fe-8ea7-4d28-c3d0-c0c46c7e5058"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Data storage and collector classes defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the classes that handle saving, loading, deduplicating, and orchestrating the entire collection process. Again, no data is fetched or saved yet."
      ],
      "metadata": {
        "id": "Pczp6Gm05BJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we create \"instances\" of our classes. We pass the API keys to the fetching classes and create the storage and collector objects."
      ],
      "metadata": {
        "id": "RN7c069b6ohH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "TICKERS = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'AMZN']\n",
        "\n",
        "# Initialize components\n",
        "price_api = StockPriceAPI(ALPHAVANTAGE_KEY)\n",
        "news_api = NewsAPI(NEWSAPI_KEY)\n",
        "storage = IncrementalDataStorage('data/raw')\n",
        "collector = DataCollector(price_api, news_api, storage)\n",
        "\n",
        "print(\"\\nâœ“ All components initialized and ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaGrKuft5BZg",
        "outputId": "37155156-c7c4-4f08-eabd-4f7b6f1e1f2f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Alpha Vantage initialized (API key: ...Z82S)\n",
            "âœ“ NewsAPI initialized (API key: ...d0ea)\n",
            "\n",
            "âœ“ All components initialized and ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the final setup step. We've defined our list of TICKERS and created the objects we'll use in the next steps."
      ],
      "metadata": {
        "id": "HtIE0LZ45GLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the status of our data files before we run the collection."
      ],
      "metadata": {
        "id": "ck-hddFt5GyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"  STATUS BEFORE COLLECTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "stats_before = storage.get_statistics()\n",
        "print(f\"  Existing price records: {stats_before['total_price_records']:,}\")\n",
        "print(f\"  Existing news articles: {stats_before['total_news_articles']:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAvZ9e-H5HEB",
        "outputId": "40e69dfd-84a1-42d0-e611-8fe5bebbb17f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "  STATUS BEFORE COLLECTION\n",
            "======================================================================\n",
            "â„¹ No existing price data found (starting fresh)\n",
            "âœ“ Loaded 460 existing news records\n",
            "  Existing price records: 0\n",
            "  Existing news articles: 460\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call the get_statistics() method on our storage object. On the very first run, this will create the data/raw directory and report that no data exists."
      ],
      "metadata": {
        "id": "JB0wYdx-5KgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the main event. We call collector.collect_and_store(). This cell will take over a minute to run because of the 13-second pause for each of the 5 tickers (5 * 13 = 65 seconds)."
      ],
      "metadata": {
        "id": "VeKEKvbA5K_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"  COLLECTING TODAY'S DATA... (This will take > 1 minute)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "new_data = collector.collect_and_store(\n",
        "    tickers=TICKERS,\n",
        "    interval='5m',\n",
        "    period='1d',      # IMPORTANT: Only get today's data!\n",
        "    news_days_back=1  # Only today's news\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"  âœ… COLLECTION COMPLETE!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x6f6Ynz5LP_",
        "outputId": "10f3b23c-dda2-45b2-b247-d29d4fc04152"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "  COLLECTING TODAY'S DATA... (This will take > 1 minute)\n",
            "======================================================================\n",
            "\n",
            "ðŸ“Š Collecting data for AAPL...\n",
            "  ... [Alpha Vantage] Waiting 13s to respect 5 calls/min rate limit...\n",
            "  âœ“ 100 price records\n",
            "  âœ“ 93 news articles\n",
            "\n",
            "ðŸ“Š Collecting data for MSFT...\n",
            "  ... [Alpha Vantage] Waiting 13s to respect 5 calls/min rate limit...\n",
            "  âœ“ 100 price records\n",
            "  âœ“ 96 news articles\n",
            "\n",
            "ðŸ“Š Collecting data for GOOGL...\n",
            "  ... [Alpha Vantage] Waiting 13s to respect 5 calls/min rate limit...\n",
            "  âœ“ 100 price records\n",
            "  âœ“ 100 news articles\n",
            "\n",
            "ðŸ“Š Collecting data for TSLA...\n",
            "  ... [Alpha Vantage] Waiting 13s to respect 5 calls/min rate limit...\n",
            "  âœ“ 100 price records\n",
            "  âœ“ 78 news articles\n",
            "\n",
            "ðŸ“Š Collecting data for AMZN...\n",
            "  ... [Alpha Vantage] Waiting 13s to respect 5 calls/min rate limit...\n",
            "  âœ“ 100 price records\n",
            "  âœ“ 96 news articles\n",
            "â„¹ No existing price data found (starting fresh)\n",
            "âœ“ Loaded 460 existing news records\n",
            "âœ“ Saved 500 total price records (added 500 new)\n",
            "âœ“ Saved 460 total news records (added 463 new)\n",
            "\n",
            "======================================================================\n",
            "  âœ… COLLECTION COMPLETE!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The collector will now loop through each ticker. For each one, it will:\n",
        "\n",
        "Call price_api.get_intraday_quotes() (pausing for 13 seconds).\n",
        "\n",
        "Call news_api.get_news().\n",
        "\n",
        "After the loop, it will call storage.append_new_data() to save the results."
      ],
      "metadata": {
        "id": "4X5dBT9r4776"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that Cell 6 has finished, let's check the stats again. The numbers should now reflect the data we just downloaded."
      ],
      "metadata": {
        "id": "Z2oQPYE95PIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"  STATUS AFTER COLLECTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "stats_after = storage.get_statistics()\n",
        "print(f\"  Total price records: {stats_after['total_price_records']:,}\")\n",
        "print(f\"  Total news articles: {stats_after['total_news_articles']:,}\")\n",
        "if stats_after.get('date_range'):\n",
        "    print(f\"  Date range: {stats_after['date_range']['start']} to {stats_after['date_range']['end']}\")\n",
        "if stats_after.get('records_per_ticker'):\n",
        "    print(\"\\n  Records per ticker:\")\n",
        "    for ticker, count in stats_after['records_per_ticker'].items():\n",
        "        print(f\"    â€¢ {ticker}: {count:,} records\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX9dSexJ5PcN",
        "outputId": "56826cb5-70a9-4241-cea3-64d4e10daef4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "  STATUS AFTER COLLECTION\n",
            "======================================================================\n",
            "âœ“ Loaded 500 existing price records\n",
            "âœ“ Loaded 460 existing news records\n",
            "  Total price records: 500\n",
            "  Total news articles: 460\n",
            "  Date range: 2025-11-07 16:40:00+00:00 to 2025-11-08 00:55:00+00:00\n",
            "\n",
            "  Records per ticker:\n",
            "    â€¢ AAPL: 100 records\n",
            "    â€¢ AMZN: 100 records\n",
            "    â€¢ GOOGL: 100 records\n",
            "    â€¢ MSFT: 100 records\n",
            "    â€¢ TSLA: 100 records\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running get_statistics() now will find the prices_master.csv and news_master.csv files, load them, and report the new counts."
      ],
      "metadata": {
        "id": "P_KRN76S5VLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The stats are great, but let's look at the actual data we saved to confirm it's correct."
      ],
      "metadata": {
        "id": "vO3z_-5F5Vdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def view_sample_data():\n",
        "    \"\"\"Helper function to view accumulated data\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"  ACCUMULATED DATA VIEWER\")\n",
        "    print(\"=\"*70)\n",
        "    storage = IncrementalDataStorage('data/raw')\n",
        "    data = storage.load_master_data()\n",
        "\n",
        "    if not data['prices'].empty:\n",
        "        print(\"\\n  ðŸ“Š Sample Price Data (latest 5 records):\")\n",
        "        print(data['prices'].tail(5)[['timestamp', 'ticker', 'close', 'volume']])\n",
        "    else:\n",
        "        print(\"\\n  â„¹ No price data to display.\")\n",
        "\n",
        "    if not data['news'].empty:\n",
        "        print(\"\\n  ðŸ“° Sample News Data (latest 5 articles):\")\n",
        "        for _, row in data['news'].tail(5).iterrows():\n",
        "            print(f\"    â€¢ [{row['ticker']}] {row['headline'][:70]}...\")\n",
        "    else:\n",
        "        print(\"\\n  â„¹ No news data to display.\")\n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "# Run the view function\n",
        "view_sample_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUtI1C5Y5Vtx",
        "outputId": "5d263264-e010-4aa0-fc75-fa9938379a75"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "  ACCUMULATED DATA VIEWER\n",
            "======================================================================\n",
            "âœ“ Loaded 500 existing price records\n",
            "âœ“ Loaded 460 existing news records\n",
            "\n",
            "  ðŸ“Š Sample Price Data (latest 5 records):\n",
            "                    timestamp ticker   close  volume\n",
            "495 2025-11-08 00:35:00+00:00   TSLA  432.23    6208\n",
            "496 2025-11-08 00:40:00+00:00   TSLA  431.99   12163\n",
            "497 2025-11-08 00:45:00+00:00   TSLA  432.00   21640\n",
            "498 2025-11-08 00:50:00+00:00   TSLA  432.05   30719\n",
            "499 2025-11-08 00:55:00+00:00   TSLA  432.03   28893\n",
            "\n",
            "  ðŸ“° Sample News Data (latest 5 articles):\n",
            "    â€¢ [TSLA] Elon Musk uses Grok to imagine the possibility of love | TechCrunch...\n",
            "    â€¢ [TSLA] Vibe-coding is now an official word in the dictionary...\n",
            "    â€¢ [TSLA] Elon Musk uses Grok to imagine the possibility of love...\n",
            "    â€¢ [TSLA] Got a spare $50,000? Cooling a single Nvidia Blackwell Ultra NVL72 rac...\n",
            "    â€¢ [TSLA] Mark Cuban Said 'I Didn't Care About The Business Side Of The Maverick...\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines and runs a simple function to load the master files and print the last 5 rows of each DataFrame. This is the best way to verify that the data looks correct."
      ],
      "metadata": {
        "id": "sIHvFPsc7SK8"
      }
    }
  ]
}