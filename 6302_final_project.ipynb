{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOU67aBFpQV930mi3KQRpmc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prithwi13/6302_stock/blob/main/6302_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aSiyJcwZgf6",
        "outputId": "05c57ede-5448-4abf-8807-259f362e7536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "  FINANCIAL SENTIMENT ANALYSIS - INCREMENTAL DATA COLLECTION\n",
            "======================================================================\n",
            "âœ“ yfinance initialized (no API key required)\n",
            "\n",
            "======================================================================\n",
            "  BEFORE COLLECTION\n",
            "======================================================================\n",
            "âœ“ Loaded 390 existing price records\n",
            "âœ“ Loaded 461 existing news records\n",
            "  Existing price records: 390\n",
            "  Existing news articles: 461\n",
            "  Date range: 2025-11-07 14:30:00+00:00 to 2025-11-07 20:55:00+00:00\n",
            "  Trading days collected: 1\n",
            "\n",
            "======================================================================\n",
            "  COLLECTING TODAY'S DATA\n",
            "======================================================================\n",
            "\n",
            "ðŸ“Š Collecting data for AAPL...\n",
            "  âœ“ 78 price records\n",
            "  âœ“ 97 news articles\n",
            "\n",
            "ðŸ“Š Collecting data for MSFT...\n",
            "  âœ“ 78 price records\n",
            "  âœ“ 99 news articles\n",
            "\n",
            "ðŸ“Š Collecting data for GOOGL...\n",
            "  âœ“ 78 price records\n",
            "  âœ“ 100 news articles\n",
            "\n",
            "ðŸ“Š Collecting data for TSLA...\n",
            "  âœ“ 78 price records\n",
            "  âœ“ 97 news articles\n",
            "\n",
            "ðŸ“Š Collecting data for AMZN...\n",
            "  âœ“ 78 price records\n",
            "  âœ“ 98 news articles\n",
            "âœ“ Loaded 390 existing price records\n",
            "âœ“ Loaded 461 existing news records\n",
            "âœ“ Saved 390 total price records (added 390 new)\n",
            "âœ“ Saved 461 total news records (added 491 new)\n",
            "\n",
            "======================================================================\n",
            "  AFTER COLLECTION\n",
            "======================================================================\n",
            "âœ“ Loaded 390 existing price records\n",
            "âœ“ Loaded 461 existing news records\n",
            "  Total price records: 390\n",
            "  Total news articles: 461\n",
            "  Date range: 2025-11-07 14:30:00+00:00 to 2025-11-07 20:55:00+00:00\n",
            "  Trading days collected: 1\n",
            "\n",
            "  Records per ticker:\n",
            "    â€¢ AAPL: 78 records\n",
            "    â€¢ AMZN: 78 records\n",
            "    â€¢ GOOGL: 78 records\n",
            "    â€¢ MSFT: 78 records\n",
            "    â€¢ TSLA: 78 records\n",
            "âœ“ Created backup: data/raw/daily_backups/prices_backup_20251107.csv\n",
            "âœ“ Created backup: data/raw/daily_backups/news_backup_20251107.csv\n",
            "\n",
            "======================================================================\n",
            "  âœ… COLLECTION COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "  Master files:\n",
            "    â€¢ data/raw/prices_master.csv\n",
            "    â€¢ data/raw/news_master.csv\n",
            "\n",
            "  Daily backups:\n",
            "    â€¢ data/raw/daily_backups/\n",
            "\n",
            "  ðŸ’¡ Run this script daily for 5-7 days to accumulate data!\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Financial Data Collection with INCREMENTAL STORAGE\n",
        "Properly accumulates data over 7 days without overwriting\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Optional\n",
        "import time\n",
        "import glob\n",
        "\n",
        "class StockPriceAPI:\n",
        "    \"\"\"Handles stock price data using yfinance (completely FREE)\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"âœ“ yfinance initialized (no API key required)\")\n",
        "\n",
        "    def get_intraday_quotes(self, ticker: str, interval='5m', period='7d') -> pd.DataFrame:\n",
        "        \"\"\"Fetch intraday quotes using yfinance\"\"\"\n",
        "        try:\n",
        "            stock = yf.Ticker(ticker)\n",
        "            df = stock.history(period=period, interval=interval)\n",
        "\n",
        "            if df.empty:\n",
        "                print(f\"âš  No data returned for {ticker}\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            df = df.reset_index()\n",
        "            df = df.rename(columns={\n",
        "                'Datetime': 'timestamp',\n",
        "                'Open': 'open',\n",
        "                'High': 'high',\n",
        "                'Low': 'low',\n",
        "                'Close': 'close',\n",
        "                'Volume': 'volume'\n",
        "            })\n",
        "\n",
        "            df['ticker'] = ticker\n",
        "            df = df[['timestamp', 'ticker', 'open', 'high', 'low', 'close', 'volume']]\n",
        "\n",
        "            if df['timestamp'].dt.tz is None:\n",
        "                df['timestamp'] = pd.to_datetime(df['timestamp']).dt.tz_localize('UTC')\n",
        "            else:\n",
        "                df['timestamp'] = pd.to_datetime(df['timestamp']).dt.tz_convert('UTC')\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âœ— Error fetching yfinance data for {ticker}: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "\n",
        "class NewsAPI:\n",
        "    \"\"\"Handles financial news from NewsAPI\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "\n",
        "    def get_news(self, ticker: str, company_name: str = None, days_back: int = 7) -> pd.DataFrame:\n",
        "        \"\"\"Fetch news articles\"\"\"\n",
        "        url = 'https://newsapi.org/v2/everything'\n",
        "        query = company_name if company_name else ticker\n",
        "        from_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
        "\n",
        "        params = {\n",
        "            'q': query,\n",
        "            'from': from_date,\n",
        "            'sortBy': 'publishedAt',\n",
        "            'language': 'en',\n",
        "            'apiKey': self.api_key,\n",
        "            'pageSize': 100\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, params=params, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if data['status'] != 'ok':\n",
        "                raise ValueError(f\"API Error: {data.get('message', 'Unknown error')}\")\n",
        "\n",
        "            records = []\n",
        "            for article in data.get('articles', []):\n",
        "                if not article.get('title'):\n",
        "                    continue\n",
        "\n",
        "                records.append({\n",
        "                    'timestamp': pd.to_datetime(article['publishedAt']).tz_convert('UTC'),\n",
        "                    'headline': article['title'],\n",
        "                    'description': article.get('description', ''),\n",
        "                    'source': article['source']['name'],\n",
        "                    'url': article['url'],\n",
        "                    'ticker': ticker\n",
        "                })\n",
        "\n",
        "            if records:\n",
        "                df = pd.DataFrame(records)\n",
        "                df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "                return df\n",
        "\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âœ— Error fetching news for {ticker}: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "\n",
        "class IncrementalDataStorage:\n",
        "    \"\"\"\n",
        "    Handles incremental data storage - KEY COMPONENT!\n",
        "    Accumulates data over time without overwriting\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_dir='data/raw'):\n",
        "        self.data_dir = data_dir\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "        # Master files that accumulate ALL data\n",
        "        self.prices_master_file = os.path.join(data_dir, 'prices_master.csv')\n",
        "        self.news_master_file = os.path.join(data_dir, 'news_master.csv')\n",
        "\n",
        "        # Daily backup folder\n",
        "        self.backup_dir = os.path.join(data_dir, 'daily_backups')\n",
        "        os.makedirs(self.backup_dir, exist_ok=True)\n",
        "\n",
        "    def load_master_data(self) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"Load existing accumulated data\"\"\"\n",
        "        prices = pd.DataFrame()\n",
        "        news = pd.DataFrame()\n",
        "\n",
        "        # Load prices if exists\n",
        "        if os.path.exists(self.prices_master_file):\n",
        "            prices = pd.read_csv(self.prices_master_file)\n",
        "            prices['timestamp'] = pd.to_datetime(prices['timestamp'])\n",
        "            print(f\"âœ“ Loaded {len(prices):,} existing price records\")\n",
        "        else:\n",
        "            print(\"â„¹ No existing price data found (starting fresh)\")\n",
        "\n",
        "        # Load news if exists\n",
        "        if os.path.exists(self.news_master_file):\n",
        "            news = pd.read_csv(self.news_master_file)\n",
        "            news['timestamp'] = pd.to_datetime(news['timestamp'])\n",
        "            print(f\"âœ“ Loaded {len(news):,} existing news records\")\n",
        "        else:\n",
        "            print(\"â„¹ No existing news data found (starting fresh)\")\n",
        "\n",
        "        return {'prices': prices, 'news': news}\n",
        "\n",
        "    def append_new_data(self, new_data: Dict[str, pd.DataFrame]):\n",
        "        \"\"\"\n",
        "        Append new data to master files (CRITICAL METHOD)\n",
        "        Handles deduplication automatically\n",
        "        \"\"\"\n",
        "        # Load existing data\n",
        "        existing = self.load_master_data()\n",
        "\n",
        "        # === PRICES ===\n",
        "        if not new_data['prices'].empty:\n",
        "            if existing['prices'].empty:\n",
        "                # First time - just save\n",
        "                combined_prices = new_data['prices']\n",
        "            else:\n",
        "                # Append new to existing\n",
        "                combined_prices = pd.concat([existing['prices'], new_data['prices']], ignore_index=True)\n",
        "\n",
        "            # Remove duplicates (same timestamp + ticker)\n",
        "            combined_prices = combined_prices.drop_duplicates(\n",
        "                subset=['timestamp', 'ticker'],\n",
        "                keep='last'  # Keep newest data\n",
        "            )\n",
        "\n",
        "            # Sort by timestamp\n",
        "            combined_prices = combined_prices.sort_values(['ticker', 'timestamp']).reset_index(drop=True)\n",
        "\n",
        "            # Save master file\n",
        "            combined_prices.to_csv(self.prices_master_file, index=False)\n",
        "            print(f\"âœ“ Saved {len(combined_prices):,} total price records (added {len(new_data['prices'])} new)\")\n",
        "\n",
        "        # === NEWS ===\n",
        "        if not new_data['news'].empty:\n",
        "            if existing['news'].empty:\n",
        "                # First time - just save\n",
        "                combined_news = new_data['news']\n",
        "            else:\n",
        "                # Append new to existing\n",
        "                combined_news = pd.concat([existing['news'], new_data['news']], ignore_index=True)\n",
        "\n",
        "            # Remove duplicates (same headline + ticker)\n",
        "            combined_news = combined_news.drop_duplicates(\n",
        "                subset=['headline', 'ticker'],\n",
        "                keep='first'  # Keep oldest (first seen)\n",
        "            )\n",
        "\n",
        "            # Sort by timestamp\n",
        "            combined_news = combined_news.sort_values(['ticker', 'timestamp']).reset_index(drop=True)\n",
        "\n",
        "            # Save master file\n",
        "            combined_news.to_csv(self.news_master_file, index=False)\n",
        "            print(f\"âœ“ Saved {len(combined_news):,} total news records (added {len(new_data['news'])} new)\")\n",
        "\n",
        "    def create_daily_backup(self):\n",
        "        \"\"\"Create a daily backup of master files\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d')\n",
        "\n",
        "        if os.path.exists(self.prices_master_file):\n",
        "            backup_file = os.path.join(self.backup_dir, f'prices_backup_{timestamp}.csv')\n",
        "            pd.read_csv(self.prices_master_file).to_csv(backup_file, index=False)\n",
        "            print(f\"âœ“ Created backup: {backup_file}\")\n",
        "\n",
        "        if os.path.exists(self.news_master_file):\n",
        "            backup_file = os.path.join(self.backup_dir, f'news_backup_{timestamp}.csv')\n",
        "            pd.read_csv(self.news_master_file).to_csv(backup_file, index=False)\n",
        "            print(f\"âœ“ Created backup: {backup_file}\")\n",
        "\n",
        "    def get_statistics(self) -> dict:\n",
        "        \"\"\"Get statistics about collected data\"\"\"\n",
        "        data = self.load_master_data()\n",
        "\n",
        "        stats = {\n",
        "            'total_price_records': len(data['prices']),\n",
        "            'total_news_articles': len(data['news']),\n",
        "            'tickers': [],\n",
        "            'date_range': None,\n",
        "            'trading_days': 0,\n",
        "            'records_per_ticker': {}\n",
        "        }\n",
        "\n",
        "        if not data['prices'].empty:\n",
        "            stats['tickers'] = data['prices']['ticker'].unique().tolist()\n",
        "            stats['date_range'] = {\n",
        "                'start': data['prices']['timestamp'].min(),\n",
        "                'end': data['prices']['timestamp'].max()\n",
        "            }\n",
        "            stats['trading_days'] = data['prices']['timestamp'].dt.date.nunique()\n",
        "            stats['records_per_ticker'] = data['prices'].groupby('ticker').size().to_dict()\n",
        "\n",
        "        return stats\n",
        "\n",
        "\n",
        "class DataCollector:\n",
        "    \"\"\"Main data collection orchestrator\"\"\"\n",
        "\n",
        "    def __init__(self, price_api: StockPriceAPI, news_api: NewsAPI, storage: IncrementalDataStorage):\n",
        "        self.price_api = price_api\n",
        "        self.news_api = news_api\n",
        "        self.storage = storage\n",
        "\n",
        "    def collect_and_store(self, tickers: List[str], interval='5m', period='1d', news_days_back=1):\n",
        "        \"\"\"\n",
        "        Collect data and store incrementally\n",
        "\n",
        "        NOTE: Use period='1d' for daily runs to get only new data!\n",
        "        \"\"\"\n",
        "        all_prices = []\n",
        "        all_news = []\n",
        "\n",
        "        company_map = {\n",
        "            'AAPL': 'Apple',\n",
        "            'MSFT': 'Microsoft',\n",
        "            'GOOGL': 'Google',\n",
        "            'AMZN': 'Amazon',\n",
        "            'TSLA': 'Tesla',\n",
        "            'NVDA': 'NVIDIA',\n",
        "            'META': 'Meta'\n",
        "        }\n",
        "\n",
        "        for ticker in tickers:\n",
        "            print(f\"\\nðŸ“Š Collecting data for {ticker}...\")\n",
        "\n",
        "            # Fetch prices for TODAY only (period='1d')\n",
        "            prices = self.price_api.get_intraday_quotes(ticker, interval, period)\n",
        "            if not prices.empty:\n",
        "                all_prices.append(prices)\n",
        "                print(f\"  âœ“ {len(prices)} price records\")\n",
        "\n",
        "            # Fetch news\n",
        "            company_name = company_map.get(ticker, ticker)\n",
        "            news = self.news_api.get_news(ticker, company_name, news_days_back)\n",
        "            if not news.empty:\n",
        "                all_news.append(news)\n",
        "                print(f\"  âœ“ {len(news)} news articles\")\n",
        "\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        # Combine today's data\n",
        "        new_data = {\n",
        "            'prices': pd.concat(all_prices, ignore_index=True) if all_prices else pd.DataFrame(),\n",
        "            'news': pd.concat(all_news, ignore_index=True) if all_news else pd.DataFrame()\n",
        "        }\n",
        "\n",
        "        # Append to master files (this is the KEY!)\n",
        "        self.storage.append_new_data(new_data)\n",
        "\n",
        "        return new_data\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Run incremental data collection\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"  FINANCIAL SENTIMENT ANALYSIS - INCREMENTAL DATA COLLECTION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Configuration\n",
        "    TICKERS = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'AMZN']\n",
        "    NEWSAPI_KEY = 'ad3d858be2a34d4e82ac459ada5bd0ea'\n",
        "\n",
        "    # Initialize components\n",
        "    price_api = StockPriceAPI()\n",
        "    news_api = NewsAPI(NEWSAPI_KEY)\n",
        "    storage = IncrementalDataStorage('data/raw')\n",
        "    collector = DataCollector(price_api, news_api, storage)\n",
        "\n",
        "    # Show current statistics BEFORE collection\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"  BEFORE COLLECTION\")\n",
        "    print(\"=\"*70)\n",
        "    stats_before = storage.get_statistics()\n",
        "    print(f\"  Existing price records: {stats_before['total_price_records']:,}\")\n",
        "    print(f\"  Existing news articles: {stats_before['total_news_articles']:,}\")\n",
        "    if stats_before['date_range']:\n",
        "        print(f\"  Date range: {stats_before['date_range']['start']} to {stats_before['date_range']['end']}\")\n",
        "        print(f\"  Trading days collected: {stats_before['trading_days']}\")\n",
        "\n",
        "    # Collect TODAY's data and append\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"  COLLECTING TODAY'S DATA\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    new_data = collector.collect_and_store(\n",
        "        tickers=TICKERS,\n",
        "        interval='5m',\n",
        "        period='1d',  # IMPORTANT: Only get today's data!\n",
        "        news_days_back=1  # Only today's news\n",
        "    )\n",
        "\n",
        "    # Show statistics AFTER collection\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"  AFTER COLLECTION\")\n",
        "    print(\"=\"*70)\n",
        "    stats_after = storage.get_statistics()\n",
        "    print(f\"  Total price records: {stats_after['total_price_records']:,}\")\n",
        "    print(f\"  Total news articles: {stats_after['total_news_articles']:,}\")\n",
        "    if stats_after['date_range']:\n",
        "        print(f\"  Date range: {stats_after['date_range']['start']} to {stats_after['date_range']['end']}\")\n",
        "        print(f\"  Trading days collected: {stats_after['trading_days']}\")\n",
        "\n",
        "    print(\"\\n  Records per ticker:\")\n",
        "    for ticker, count in stats_after['records_per_ticker'].items():\n",
        "        print(f\"    â€¢ {ticker}: {count:,} records\")\n",
        "\n",
        "    # Create daily backup\n",
        "    storage.create_daily_backup()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"  âœ… COLLECTION COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\n  Master files:\")\n",
        "    print(f\"    â€¢ data/raw/prices_master.csv\")\n",
        "    print(f\"    â€¢ data/raw/news_master.csv\")\n",
        "    print(\"\\n  Daily backups:\")\n",
        "    print(f\"    â€¢ data/raw/daily_backups/\")\n",
        "    print(\"\\n  ðŸ’¡ Run this script daily for 5-7 days to accumulate data!\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "\n",
        "def view_data():\n",
        "    \"\"\"Helper function to view accumulated data\"\"\"\n",
        "    storage = IncrementalDataStorage('data/raw')\n",
        "    data = storage.load_master_data()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"  ACCUMULATED DATA VIEWER\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    stats = storage.get_statistics()\n",
        "    print(f\"\\n  Total Records:\")\n",
        "    print(f\"    â€¢ Prices: {stats['total_price_records']:,}\")\n",
        "    print(f\"    â€¢ News: {stats['total_news_articles']:,}\")\n",
        "    print(f\"    â€¢ Tickers: {', '.join(stats['tickers'])}\")\n",
        "    print(f\"    â€¢ Trading days: {stats['trading_days']}\")\n",
        "\n",
        "    if stats['date_range']:\n",
        "        print(f\"\\n  Date Range:\")\n",
        "        print(f\"    â€¢ Start: {stats['date_range']['start']}\")\n",
        "        print(f\"    â€¢ End: {stats['date_range']['end']}\")\n",
        "\n",
        "    print(f\"\\n  Records per ticker:\")\n",
        "    for ticker, count in stats['records_per_ticker'].items():\n",
        "        print(f\"    â€¢ {ticker}: {count:,}\")\n",
        "\n",
        "    # Show sample data\n",
        "    if not data['prices'].empty:\n",
        "        print(\"\\n  ðŸ“Š Sample Price Data (latest 5 records):\")\n",
        "        print(data['prices'].tail(5)[['timestamp', 'ticker', 'close', 'volume']])\n",
        "\n",
        "    if not data['news'].empty:\n",
        "        print(\"\\n  ðŸ“° Sample News Data (latest 5 articles):\")\n",
        "        for _, row in data['news'].tail(5).iterrows():\n",
        "            print(f\"    â€¢ [{row['ticker']}] {row['headline'][:60]}...\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import sys\n",
        "\n",
        "    if len(sys.argv) > 1 and sys.argv[1] == 'view':\n",
        "        # View accumulated data: python script.py view\n",
        "        view_data()\n",
        "    else:\n",
        "        # Run collection: python script.py\n",
        "        main()"
      ]
    }
  ]
}