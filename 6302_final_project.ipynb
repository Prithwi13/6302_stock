{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prithwi13/6302_stock/blob/main/6302_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "first cell imports all the necessary libraries for your project. Most importantly, it securely loads your API keys from the Colab \"Secrets\" (ðŸ”‘) tab."
      ],
      "metadata": {
        "id": "Lu2t6ryL6P2R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aSiyJcwZgf6",
        "outputId": "befcac6a-a6a3-48e5-c99c-6f46015c95f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ API keys loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Optional\n",
        "import time\n",
        "import sys\n",
        "from google.colab import userdata\n",
        "\n",
        "# === SECURELY Load API keys from Colab Secrets ===\n",
        "try:\n",
        "    ALPHAVANTAGE_KEY = userdata.get('ALPHAVANTAGE_KEY')\n",
        "    NEWSAPI_KEY = userdata.get('NEWSAPI_KEY')\n",
        "    print(\"âœ“ API keys loaded successfully.\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"âœ— FATAL ERROR: API Key not found in Colab Secrets.\")\n",
        "    print(\"  Please click the 'Key' icon (ðŸ”‘) on the left sidebar,\")\n",
        "    print(\"  add 'ALPHAVANTAGE_KEY' and 'NEWSAPI_KEY',\")\n",
        "    print(\"  and make sure 'Notebook access' is toggled ON for both.\")\n",
        "    sys.exit(1) # Stop execution\n",
        "except Exception as e:\n",
        "    print(f\"âœ— FATAL ERROR: Could not load secrets. {e}\")\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code runs the import statements and then immediately tries to fetch your keys from the Colab Secrets manager. If it fails, it prints a helpful error message and stops the script."
      ],
      "metadata": {
        "id": "T8PlcLh240-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we define the classes responsible for fetching data from the two external APIs. These are your \"worker\" classes."
      ],
      "metadata": {
        "id": "wHbYZuPh6XEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StockPriceAPI:\n",
        "    \"\"\"Handles stock price data using Alpha Vantage\"\"\"\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        self.base_url = \"https://www.alphavantage.co/query\"\n",
        "        print(f\"âœ“ Alpha Vantage initialized (API key: ...{api_key[-4:]})\")\n",
        "\n",
        "    def get_intraday_quotes(self, ticker: str, interval='5m', period='7d') -> pd.DataFrame:\n",
        "        print(f\"  ... [Alpha Vantage] Waiting 13s to respect 5 calls/min rate limit...\")\n",
        "        time.sleep(13)\n",
        "        av_interval = interval.replace('m', 'min')\n",
        "        av_outputsize = 'compact' if period == '1d' else 'full'\n",
        "        params = {\n",
        "            'function': 'TIME_SERIES_INTRADAY', 'symbol': ticker, 'interval': av_interval,\n",
        "            'outputsize': av_outputsize, 'apikey': self.api_key, 'datatype': 'json'\n",
        "        }\n",
        "        try:\n",
        "            response = requests.get(self.base_url, params=params, timeout=20)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "            if 'Error Message' in data:\n",
        "                print(f\"âœ— [Alpha Vantage] API Error for {ticker}: {data['Error Message']}\")\n",
        "                return pd.DataFrame()\n",
        "            if 'Note' in data:\n",
        "                print(f\"âœ— [Alpha Vantage] API Note for {ticker}: {data['Note']}\")\n",
        "                return pd.DataFrame()\n",
        "            data_key = next((key for key in data.keys() if 'Time Series' in key), None)\n",
        "            if data_key is None:\n",
        "                print(f\"âœ— [Alpha Vantage] Could not find 'Time Series' data key for {ticker}.\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            df = pd.DataFrame.from_dict(data[data_key], orient='index')\n",
        "            if df.empty: return pd.DataFrame()\n",
        "\n",
        "            df = df.reset_index().rename(columns={\n",
        "                'index': 'timestamp', '1. open': 'open', '2. high': 'high',\n",
        "                '3. low': 'low', '4. close': 'close', '5. volume': 'volume'\n",
        "            })\n",
        "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "            for col in ['open', 'high', 'low', 'close', 'volume']:\n",
        "                df[col] = pd.to_numeric(df[col])\n",
        "            df['ticker'] = ticker\n",
        "            try:\n",
        "                df['timestamp'] = df['timestamp'].dt.tz_localize('America/New_York').dt.tz_convert('UTC')\n",
        "            except Exception:\n",
        "                df['timestamp'] = df['timestamp'].dt.tz_localize('UTC') # Fallback\n",
        "            df = df[['timestamp', 'ticker', 'open', 'high', 'low', 'close', 'volume']]\n",
        "            return df.sort_values('timestamp').reset_index(drop=True)\n",
        "        except Exception as e:\n",
        "            print(f\"âœ— Error fetching Alpha Vantage data for {ticker}: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "class MarketAuxAPI:\n",
        "    \"\"\"Handles financial news from MarketAux\"\"\"\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        self.base_url = \"https://api.marketaux.com/v1/news/all\"\n",
        "        print(f\"âœ“ MarketAuxAPI initialized (API key: ...{api_key[-4:]})\")\n",
        "\n",
        "    def _parse_articles(self, articles: List[Dict], ticker: str) -> pd.DataFrame:\n",
        "        \"\"\"Helper to parse article JSON into a DataFrame.\"\"\"\n",
        "        records = []\n",
        "        for article in articles:\n",
        "            records.append({\n",
        "                'timestamp': pd.to_datetime(article['published_at']).tz_convert('UTC'),\n",
        "                'headline': article['title'],\n",
        "                'description': article.get('description', '') or article.get('snippet', ''),\n",
        "                'source': article.get('source', 'unknown'),\n",
        "                'url': article['url'],\n",
        "                'ticker': ticker\n",
        "            })\n",
        "        df = pd.DataFrame(records)\n",
        "        if not df.empty:\n",
        "            df['timestamp'] = df['timestamp'].dt.normalize()\n",
        "        return df\n",
        "\n",
        "    def get_news_incremental(self, ticker: str) -> pd.DataFrame:\n",
        "        \"\"\"Gets news from the last 3 days for daily updates.\"\"\"\n",
        "        print(f\"  ... [MarketAux] Fetching recent news for {ticker}...\")\n",
        "        date_to = datetime.now()\n",
        "        date_from = date_to - timedelta(days=3)\n",
        "\n",
        "        params = {\n",
        "            'api_token': self.api_key,\n",
        "            'symbols': ticker,\n",
        "            'language': 'en',\n",
        "            'published_after': date_from.strftime('%Y-%m-%dT%H:%M:%S'),\n",
        "\n",
        "            # --- THIS IS THE LIMIT YOU ASKED ABOUT ---\n",
        "            # This asks the API for the max 50 recent articles.\n",
        "            # If we remove this, it will default to 10 or 25.\n",
        "            'limit': 50\n",
        "        }\n",
        "        try:\n",
        "            response = requests.get(self.base_url, params=params, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "            articles = data.get('data', [])\n",
        "            return self._parse_articles(articles, ticker)\n",
        "        except Exception as e:\n",
        "            print(f\"âœ— Error fetching MarketAux incremental news for {ticker}: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def get_news_backfill(self, ticker: str, start_date: str, end_date: str) -> pd.DataFrame:\n",
        "        \"\"\"Gets historical news for a given date range. Loops by month.\"\"\"\n",
        "        print(f\"  ... [MarketAux] Backfilling news for {ticker} from {start_date} to {end_date}...\")\n",
        "        all_articles = []\n",
        "        current_date = pd.to_datetime(start_date)\n",
        "        end_date_dt = pd.to_datetime(end_date)\n",
        "\n",
        "        while current_date <= end_date_dt:\n",
        "            month_start = current_date.strftime('%Y-%m-01')\n",
        "            month_end_dt = (current_date + pd.offsets.MonthEnd(1))\n",
        "            if month_end_dt > end_date_dt:\n",
        "                month_end_dt = end_date_dt\n",
        "            month_end = month_end_dt.strftime('%Y-%m-%d')\n",
        "\n",
        "            print(f\"    ... Fetching {ticker} news for {month_start} to {month_end}\")\n",
        "            params = {\n",
        "                'api_token': self.api_key,\n",
        "                'symbols': ticker,\n",
        "                'language': 'en',\n",
        "                'published_after': f\"{month_start}T00:00:00\",\n",
        "                'published_before': f\"{month_end}T23:59:59\",\n",
        "\n",
        "                # --- THIS IS THE OTHER LIMIT ---\n",
        "                # This asks the API for the max 100 articles *for this month*.\n",
        "                # This is the highest number the free plan allows per request.\n",
        "                'limit': 100\n",
        "            }\n",
        "            try:\n",
        "                response = requests.get(self.base_url, params=params, timeout=20)\n",
        "                response.raise_for_status()\n",
        "                data = response.json()\n",
        "                articles = data.get('data', [])\n",
        "                all_articles.extend(articles)\n",
        "                time.sleep(13) # Wait to avoid rate limiting (5 req/min)\n",
        "            except Exception as e:\n",
        "                print(f\"âœ— Error fetching MarketAux backfill for {ticker} ({month_start}): {e}\")\n",
        "\n",
        "            current_date = current_date + pd.offsets.MonthBegin(1)\n",
        "\n",
        "        return self._parse_articles(all_articles, ticker)"
      ],
      "metadata": {
        "id": "Bp6mV0gQ41ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the classes. It doesn't run them yet, it just makes them available for later use. Note the critical time.sleep(13) in StockPriceAPIâ€”this is essential for not getting blocked by the free API."
      ],
      "metadata": {
        "id": "x8i0_m1O48KH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These classes are the \"brains\" of the operation. IncrementalDataStorage is the most important partâ€”it handles saving data and preventing duplicates. DataCollector manages the entire process."
      ],
      "metadata": {
        "id": "4y3haFEu48hD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Optional\n",
        "import time\n",
        "\n",
        "# --- NOTE ---\n",
        "# This code fragment assumes you have already defined the\n",
        "# 'StockPriceAPI' and 'MarketAuxAPI' classes in a cell above this one.\n",
        "# (You can get them from the 'stock_pipeline_classes.py' file)\n",
        "\n",
        "\n",
        "# --- THIS IS THE CORRECTED STORAGE CLASS (with backfill fix) ---\n",
        "class IncrementalDataStorage:\n",
        "    \"\"\"Handles incremental data storage and deduplication.\"\"\"\n",
        "    def __init__(self, data_dir='data/raw'):\n",
        "        self.data_dir = data_dir\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "        self.prices_master_file = os.path.join(data_dir, 'prices_master.csv')\n",
        "        self.news_master_file = os.path.join(data_dir, 'news_master.csv')\n",
        "        print(f\"âœ“ IncrementalDataStorage initialized.\")\n",
        "        print(f\"  ... Price file: {self.prices_master_file}\")\n",
        "        print(f\"  ... News file: {self.news_master_file}\")\n",
        "\n",
        "    def load_master_data(self) -> Dict[str, pd.DataFrame]:\n",
        "        prices = pd.DataFrame()\n",
        "        news = pd.DataFrame()\n",
        "        if os.path.exists(self.prices_master_file):\n",
        "            prices = pd.read_csv(self.prices_master_file)\n",
        "            prices['timestamp'] = pd.to_datetime(prices['timestamp'], utc=True)\n",
        "            print(f\"âœ“ Loaded {len(prices):,} existing price records\")\n",
        "        else:\n",
        "            print(\"â„¹ No existing price data found (starting fresh)\")\n",
        "        if os.path.exists(self.news_master_file):\n",
        "            news = pd.read_csv(self.news_master_file)\n",
        "            news['timestamp'] = pd.to_datetime(news['timestamp'], utc=True)\n",
        "            print(f\"âœ“ Loaded {len(news):,} existing news records\")\n",
        "        else:\n",
        "            print(\"â„¹ No existing news data found (starting fresh)\")\n",
        "        return {'prices': prices, 'news': news}\n",
        "\n",
        "    def append_new_data(self, new_data: Dict[str, pd.DataFrame], is_backfill: bool = False):\n",
        "        \"\"\"\n",
        "        Appends new data, handling backfills and incremental updates.\n",
        "        This is the fixed version.\n",
        "        \"\"\"\n",
        "        existing = self.load_master_data()\n",
        "\n",
        "        if 'prices' in new_data and not new_data['prices'].empty:\n",
        "            if is_backfill:\n",
        "                print(\"... Backfill detected. Overwriting price data.\")\n",
        "                existing['prices'] = pd.DataFrame(columns=new_data['prices'].columns)\n",
        "\n",
        "            combined_prices = pd.concat([existing['prices'], new_data['prices']], ignore_index=True)\n",
        "            combined_prices['timestamp'] = pd.to_datetime(combined_prices['timestamp'], utc=True)\n",
        "            combined_prices = combined_prices.drop_duplicates(subset=['timestamp', 'ticker'], keep='last')\n",
        "            combined_prices = combined_prices.sort_values(['ticker', 'timestamp']).reset_index(drop=True)\n",
        "            combined_prices.to_csv(self.prices_master_file, index=False)\n",
        "            added_count = len(combined_prices) - len(existing['prices'])\n",
        "            print(f\"âœ“ Saved {len(combined_prices):,} total price records (added {added_count} new)\")\n",
        "\n",
        "        if 'news' in new_data and not new_data['news'].empty:\n",
        "            if is_backfill:\n",
        "                print(\"... Backfill detected. Overwriting news data.\")\n",
        "                existing['news'] = pd.DataFrame(columns=new_data['news'].columns)\n",
        "\n",
        "            combined_news = pd.concat([existing['news'], new_data['news']], ignore_index=True)\n",
        "            combined_news['timestamp'] = pd.to_datetime(combined_news['timestamp'], utc=True)\n",
        "            combined_news = combined_news.drop_duplicates(subset=['headline', 'ticker', 'timestamp'], keep='first')\n",
        "            combined_news = combined_news.sort_values(['ticker', 'timestamp']).reset_index(drop=True)\n",
        "            combined_news.to_csv(self.news_master_file, index=False)\n",
        "            added_count = len(combined_news) - len(existing['news'])\n",
        "            print(f\"âœ“ Saved {len(combined_news):,} total news records (added {added_count} new)\")\n",
        "\n",
        "    def get_statistics(self) -> dict:\n",
        "        data = self.load_master_data()\n",
        "        stats = {'total_price_records': len(data['prices']), 'total_news_articles': len(data['news']), 'tickers': [], 'date_range': None}\n",
        "        if not data['prices'].empty:\n",
        "            stats['tickers'] = data['prices']['ticker'].unique().tolist()\n",
        "            stats['date_range'] = {'start': data['prices']['timestamp'].min(), 'end': data['prices']['timestamp'].max()}\n",
        "            stats['records_per_ticker'] = data['prices'].groupby('ticker').size().to_dict()\n",
        "        return stats\n",
        "# --- END OF CORRECTED STORAGE CLASS ---\n",
        "\n",
        "\n",
        "# --- THIS IS THE CORRECTED DATA COLLECTOR CLASS ---\n",
        "class DataCollector:\n",
        "    \"\"\"Main data collection orchestrator\"\"\"\n",
        "\n",
        "    # --- FIX: It now accepts MarketAuxAPI, not the old NewsAPI ---\n",
        "    def __init__(self, price_api: 'StockPriceAPI', news_api: 'MarketAuxAPI', storage: IncrementalDataStorage):\n",
        "        self.price_api = price_api\n",
        "        self.news_api = news_api\n",
        "        self.storage = storage\n",
        "        print(\"âœ“ DataCollector initialized.\")\n",
        "\n",
        "    def run_collection(self, tickers: List[str], collection_type: str, backfill_start_date: str):\n",
        "        \"\"\"\n",
        "        This method is built for DAILY data and works with MarketAux.\n",
        "        (It does not use 'interval' or 'period'.)\n",
        "        \"\"\"\n",
        "        all_prices, all_news = [], []\n",
        "        is_backfill = (collection_type == 'backfill')\n",
        "\n",
        "        # --- 1. Price Collection (for DAILY data) ---\n",
        "        for ticker in tickers:\n",
        "            print(f\"\\nðŸ“Š Collecting prices for {ticker}...\")\n",
        "            if is_backfill:\n",
        "                prices = self.price_api.get_daily_quotes_backfill(ticker)\n",
        "            else: # 'incremental'\n",
        "                prices = self.price_api.get_daily_quotes_incremental(ticker)\n",
        "\n",
        "            if not prices.empty:\n",
        "                all_prices.append(prices)\n",
        "                print(f\"  âœ“ Got {len(prices)} price records\")\n",
        "\n",
        "        if not all_prices:\n",
        "            print(\"No price data fetched. Skipping news collection.\")\n",
        "            return\n",
        "\n",
        "        all_prices_df = pd.concat(all_prices, ignore_index=True)\n",
        "\n",
        "        # --- 2. News Collection (for DAILY data) ---\n",
        "        min_price_date_str = all_prices_df['timestamp'].min().strftime('%Y-%m-%d')\n",
        "        max_price_date_str = all_prices_df['timestamp'].max().strftime('%Y-%m-%d')\n",
        "\n",
        "        # Use the user's requested start date, but don't ask for news\n",
        "        # older than our oldest price data.\n",
        "        final_backfill_start_date = max(min_price_date_str, backfill_start_date)\n",
        "\n",
        "        for ticker in tickers:\n",
        "            print(f\"\\nðŸ“° Collecting news for {ticker}...\")\n",
        "            if is_backfill:\n",
        "                print(f\"  ... Backfilling news from {final_backfill_start_date} to {max_price_date_str}\")\n",
        "                news = self.news_api.get_news_backfill(ticker, final_backfill_start_date, max_price_date_str)\n",
        "            else: # 'incremental'\n",
        "                news = self.news_api.get_news_incremental(ticker)\n",
        "\n",
        "            if not news.empty:\n",
        "                all_news.append(news)\n",
        "                print(f\"  âœ“ Got {len(news)} news articles\")\n",
        "\n",
        "        # --- 3. Storage ---\n",
        "        new_data = {\n",
        "            'prices': all_prices_df,\n",
        "            'news': pd.concat(all_news, ignore_index=True) if all_news else pd.DataFrame()\n",
        "        }\n",
        "        print(\"\\nðŸ’¾ Saving data to master files...\")\n",
        "        self.storage.append_new_data(new_data, is_backfill=is_backfill)\n",
        "        return new_data\n",
        "\n",
        "print(\"âœ“ Data storage and collector classes defined.\")\n",
        "# --- END OF CORRECTED DATA COLLECTOR ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrRoAQAA487D",
        "outputId": "80fc928f-b50a-4596-c7d5-60298cbdbc1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Data storage and collector classes defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the classes that handle saving, loading, deduplicating, and orchestrating the entire collection process. Again, no data is fetched or saved yet."
      ],
      "metadata": {
        "id": "Pczp6Gm05BJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we create \"instances\" of our classes. We pass the API keys to the fetching classes and create the storage and collector objects."
      ],
      "metadata": {
        "id": "RN7c069b6ohH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configuration\n",
        "TICKERS = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'AMZN']\n",
        "\n",
        "# --- FIX: Load the CORRECT API keys ---\n",
        "# We need ALPHAVANTAGE_KEY and MARKETAUX_KEY\n",
        "try:\n",
        "    ALPHAVANTAGE_KEY = userdata.get('ALPHAVANTAGE_KEY')\n",
        "    MARKETAUX_KEY = userdata.get('MARKETAUX_KEY') # <-- Use the new key\n",
        "    print(\"âœ“ API keys loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"âœ— FATAL: Could not load keys. Make sure ALPHAVANTAGE_KEY and MARKETAUX_KEY are in Colab Secrets. Error: {e}\")\n",
        "    # sys.exit(1) # This might stop your Colab session, you can comment it out if you prefer\n",
        "    raise e # A better way to show the error in Colab\n",
        "\n",
        "# --- FIX: Initialize the CORRECT classes ---\n",
        "# (These classes must be defined in a cell *before* this one.\n",
        "#  You can get them from the 'stock_pipeline_classes.py' file)\n",
        "\n",
        "price_api = StockPriceAPI(ALPHAVANTAGE_KEY)\n",
        "\n",
        "# --- This is the main fix: ---\n",
        "# 1. Use MarketAuxAPI, not NewsAPI\n",
        "# 2. Pass it MARKETAUX_KEY,\n",
        "news_api = MarketAuxAPI(MARKETAUX_KEY)\n",
        "storage = IncrementalDataStorage('data/raw')\n",
        "collector = DataCollector(price_api, news_api, storage)\n",
        "\n",
        "print(\"\\nâœ“ All components initialized and ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaGrKuft5BZg",
        "outputId": "fed9ea33-da47-4b3d-c802-dc95c28566ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ API keys loaded successfully.\n",
            "âœ“ Alpha Vantage initialized (API key: ...Z82S)\n",
            "âœ“ MarketAuxAPI initialized (API key: ...EtV7)\n",
            "âœ“ IncrementalDataStorage initialized.\n",
            "  ... Price file: data/raw/prices_master.csv\n",
            "  ... News file: data/raw/news_master.csv\n",
            "âœ“ DataCollector initialized.\n",
            "\n",
            "âœ“ All components initialized and ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the final setup step. We've defined our list of TICKERS and created the objects we'll use in the next steps."
      ],
      "metadata": {
        "id": "HtIE0LZ45GLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the status of our data files before we run the collection."
      ],
      "metadata": {
        "id": "ck-hddFt5GyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"  STATUS BEFORE COLLECTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "stats_before = storage.get_statistics()\n",
        "print(f\"  Existing price records: {stats_before['total_price_records']:,}\")\n",
        "print(f\"  Existing news articles: {stats_before['total_news_articles']:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAvZ9e-H5HEB",
        "outputId": "ac600210-f3e2-4c98-e1f3-86fbfc0670b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "  STATUS BEFORE COLLECTION\n",
            "======================================================================\n",
            "âœ“ Loaded 16,996 existing price records\n",
            "âœ“ Loaded 669 existing news records\n",
            "  Existing price records: 16,996\n",
            "  Existing news articles: 669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call the get_statistics() method on our storage object. On the very first run, this will create the data/raw directory and report that no data exists."
      ],
      "metadata": {
        "id": "JB0wYdx-5KgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the main event. We call collector.collect_and_store(). This cell will take over a minute to run because of the 13-second pause for each of the 5 tickers (5 * 13 = 65 seconds)."
      ],
      "metadata": {
        "id": "VeKEKvbA5K_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_data_collection(collection_type: str):\n",
        "    \"\"\"Main function to run the data collection step.\"\"\"\n",
        "\n",
        "    TICKERS = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'AMZN']\n",
        "\n",
        "    # --- FIX for \"5 YEARS\" ---\n",
        "    # This calculates the date 5 years ago from today.\n",
        "    FIVE_YEARS_AGO = (datetime.now() - timedelta(days=5*365)).strftime('%Y-%m-%d')\n",
        "\n",
        "    # Initialize components\n",
        "    try:\n",
        "        # Load keys from Colab Secrets\n",
        "        ALPHAVANTAGE_KEY = userdata.get('ALPHAVANTAGE_KEY')\n",
        "        MARKETAUX_KEY = userdata.get('MARKETAUX_KEY')\n",
        "\n",
        "        price_api = StockPriceAPI(ALPHAVANTAGE_KEY)\n",
        "        news_api = MarketAuxAPI(MARKETAUX_KEY)\n",
        "        storage = IncrementalDataStorage('data/raw')\n",
        "        collector = DataCollector(price_api, news_api, storage)\n",
        "        print(\"\\nâœ“ All components initialized and ready.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— FATAL: Could not initialize components. Check API keys. Error: {e}\")\n",
        "        return\n",
        "\n",
        "    if collection_type == 'backfill':\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"  RUNNING ONE-TIME DATA BACKFILL (Targeting start date: {FIVE_YEARS_AGO})\")\n",
        "        print(\"  This will take several minutes...\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # --- THIS IS THE FIX ---\n",
        "        # 1. Call the new function: `run_collection`\n",
        "        # 2. Use the new parameters: `collection_type` and `backfill_start_date`\n",
        "        collector.run_collection(\n",
        "            tickers=TICKERS,\n",
        "            collection_type='backfill',\n",
        "            backfill_start_date=FIVE_YEARS_AGO # Use our 5-year variable\n",
        "        )\n",
        "        # --- END OF FIX ---\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"  âœ… BACKFILL COMPLETE!\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "    else: # 'incremental'\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"  RUNNING DAILY INCREMENTAL UPDATE...\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # --- THIS IS THE FIX ---\n",
        "        collector.run_collection(\n",
        "            tickers=TICKERS,\n",
        "            collection_type='incremental',\n",
        "            backfill_start_date=FIVE_YEARS_AGO # Pass our 5-year variable\n",
        "        )\n",
        "        # --- END OF FIX ---\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"  âœ… DAILY UPDATE COMPLETE!\")\n",
        "        print(\"=\"*70)\n",
        "\n"
      ],
      "metadata": {
        "id": "-x6f6Ynz5LP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The collector will now loop through each ticker. For each one, it will:\n",
        "\n",
        "Call price_api.get_intraday_quotes() (pausing for 13 seconds).\n",
        "\n",
        "Call news_api.get_news().\n",
        "\n",
        "After the loop, it will call storage.append_new_data() to save the results."
      ],
      "metadata": {
        "id": "4X5dBT9r4776"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that Cell 6 has finished, let's check the stats again. The numbers should now reflect the data we just downloaded."
      ],
      "metadata": {
        "id": "Z2oQPYE95PIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"  STATUS AFTER COLLECTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "stats_after = storage.get_statistics()\n",
        "print(f\"  Total price records: {stats_after['total_price_records']:,}\")\n",
        "print(f\"  Total news articles: {stats_after['total_news_articles']:,}\")\n",
        "if stats_after.get('date_range'):\n",
        "    print(f\"  Date range: {stats_after['date_range']['start']} to {stats_after['date_range']['end']}\")\n",
        "if stats_after.get('records_per_ticker'):\n",
        "    print(\"\\n  Records per ticker:\")\n",
        "    for ticker, count in stats_after['records_per_ticker'].items():\n",
        "        print(f\"    â€¢ {ticker}: {count:,} records\")"
      ],
      "metadata": {
        "id": "rX9dSexJ5PcN",
        "outputId": "148812bd-6af1-425e-8b13-b6f266490b94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "  STATUS AFTER COLLECTION\n",
            "======================================================================\n",
            "âœ“ Loaded 16,996 existing price records\n",
            "âœ“ Loaded 669 existing news records\n",
            "  Total price records: 16,996\n",
            "  Total news articles: 669\n",
            "  Date range: 2025-10-13 08:00:00+00:00 to 2025-11-12 00:55:00+00:00\n",
            "\n",
            "  Records per ticker:\n",
            "    â€¢ AAPL: 4,224 records\n",
            "    â€¢ AMZN: 4,224 records\n",
            "    â€¢ GOOGL: 4,224 records\n",
            "    â€¢ MSFT: 100 records\n",
            "    â€¢ TSLA: 4,224 records\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running get_statistics() now will find the prices_master.csv and news_master.csv files, load them, and report the new counts."
      ],
      "metadata": {
        "id": "P_KRN76S5VLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The stats are great, but let's look at the actual data we saved to confirm it's correct."
      ],
      "metadata": {
        "id": "vO3z_-5F5Vdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def view_sample_data():\n",
        "    \"\"\"Helper function to view accumulated data\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"  ACCUMULATED DATA VIEWER\")\n",
        "    print(\"=\"*70)\n",
        "    storage = IncrementalDataStorage('data/raw')\n",
        "    data = storage.load_master_data()\n",
        "\n",
        "    if not data['prices'].empty:\n",
        "        print(\"\\n  ðŸ“Š Sample Price Data (latest 5 records):\")\n",
        "        print(data['prices'].tail(5)[['timestamp', 'ticker', 'close', 'volume']])\n",
        "    else:\n",
        "        print(\"\\n  â„¹ No price data to display.\")\n",
        "\n",
        "    if not data['news'].empty:\n",
        "        print(\"\\n  ðŸ“° Sample News Data (latest 5 articles):\")\n",
        "        for _, row in data['news'].tail(5).iterrows():\n",
        "            print(f\"    â€¢ [{row['ticker']}] {row['headline'][:70]}...\")\n",
        "    else:\n",
        "        print(\"\\n  â„¹ No news data to display.\")\n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "# Run the view function\n",
        "view_sample_data()"
      ],
      "metadata": {
        "id": "cUtI1C5Y5Vtx",
        "outputId": "1b619ce1-c6fc-49a9-8bb1-9718640f84d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "  ACCUMULATED DATA VIEWER\n",
            "======================================================================\n",
            "âœ“ IncrementalDataStorage initialized.\n",
            "  ... Price file: data/raw/prices_master.csv\n",
            "  ... News file: data/raw/news_master.csv\n",
            "âœ“ Loaded 16,996 existing price records\n",
            "âœ“ Loaded 669 existing news records\n",
            "\n",
            "  ðŸ“Š Sample Price Data (latest 5 records):\n",
            "                      timestamp ticker    close  volume\n",
            "16991 2025-11-12 00:35:00+00:00   TSLA  439.110    6334\n",
            "16992 2025-11-12 00:40:00+00:00   TSLA  439.190    2198\n",
            "16993 2025-11-12 00:45:00+00:00   TSLA  439.190    9551\n",
            "16994 2025-11-12 00:50:00+00:00   TSLA  439.245   13443\n",
            "16995 2025-11-12 00:55:00+00:00   TSLA  439.290   15912\n",
            "\n",
            "  ðŸ“° Sample News Data (latest 5 articles):\n",
            "    â€¢ [TSLA] Ford â€˜canâ€™t walk away from EVsâ€™ or it risks falling even further behin...\n",
            "    â€¢ [TSLA] Did Trump say Kamala Harris would cause 'bread lines'?...\n",
            "    â€¢ [TSLA] Tesla's First Semi Vehicle Customer After Full Production Launch Isâ€¦.T...\n",
            "    â€¢ [TSLA] Trump ally takes charge at controversial Israeli spy technology compan...\n",
            "    â€¢ [TSLA] Elon Musk Vs. Joyce Carol Oatesâ€”The Online Feud, Explained...\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines and runs a simple function to load the master files and print the last 5 rows of each DataFrame. This is the best way to verify that the data looks correct."
      ],
      "metadata": {
        "id": "sIHvFPsc7SK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "99ksk3e3QHs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Optional\n",
        "import time\n",
        "import sys\n",
        "\n",
        "# Try to import userdata, otherwise fall back to environment variables\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    ALPHAVANTAGE_KEY = userdata.get('ALPHAVANTAGE_KEY')\n",
        "    NEWSAPI_KEY = userdata.get('NEWSAPI_KEY')\n",
        "    print(\"âœ“ Colab userdata keys loaded.\")\n",
        "except (ImportError, ModuleNotFoundError):\n",
        "    print(\"Not in Colab. Trying environment variables...\")\n",
        "    ALPHAVANTAGE_KEY = os.environ.get('ALPHAVANTAGE_KEY')\n",
        "    NEWSAPI_KEY = os.environ.get('NEWSAPI_KEY')\n",
        "    if not ALPHAVANTAGE_KEY or not NEWSAPI_KEY:\n",
        "        print(\"âœ— FATAL ERROR: API keys not found in environment variables.\")\n",
        "        print(\"  Please set ALPHAVANTAGE_KEY and NEWSAPI_KEY.\")\n",
        "        sys.exit(1)\n",
        "    print(\"âœ“ Environment variable keys loaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"âœ— FATAL ERROR: Could not load secrets. {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# === DATA FETCHING CLASSES (Copied from your notebook) ===\n",
        "\n",
        "class StockPriceAPI:\n",
        "    \"\"\"Handles stock price data using Alpha Vantage\"\"\"\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        self.base_url = \"https://www.alphavantage.co/query\"\n",
        "        print(f\"âœ“ Alpha Vantage initialized (API key: ...{api_key[-4:]})\")\n",
        "\n",
        "    def get_intraday_quotes(self, ticker: str, interval='5m', period='7d') -> pd.DataFrame:\n",
        "        print(f\"  ... [Alpha Vantage] Waiting 13s to respect 5 calls/min rate limit...\")\n",
        "        time.sleep(13)\n",
        "        av_interval = interval.replace('m', 'min')\n",
        "\n",
        "        # --- THIS IS THE KEY CHANGE ---\n",
        "        # We are passing a period != '1d', so this will set av_outputsize to 'full'\n",
        "        av_outputsize = 'compact' if period == '1d' else 'full'\n",
        "        print(f\"  ... Requesting 'outputsize={av_outputsize}'\")\n",
        "        # --- END OF CHANGE ---\n",
        "\n",
        "        params = {\n",
        "            'function': 'TIME_SERIES_INTRADAY', 'symbol': ticker, 'interval': av_interval,\n",
        "            'outputsize': av_outputsize, 'apikey': self.api_key, 'datatype': 'json'\n",
        "        }\n",
        "        try:\n",
        "            response = requests.get(self.base_url, params=params, timeout=20)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "            if 'Error Message' in data:\n",
        "                print(f\"âœ— [Alpha Vantage] API Error for {ticker}: {data['Error Message']}\")\n",
        "                return pd.DataFrame()\n",
        "            if 'Note' in data:\n",
        "                print(f\"âœ— [Alpha Vantage] API Note for {ticker}: {data['Note']}\")\n",
        "                return pd.DataFrame()\n",
        "            data_key = next((key for key in data.keys() if 'Time Series' in key), None)\n",
        "            if data_key is None:\n",
        "                print(f\"âœ— [Alpha Vantage] Could not find 'Time Series' data key for {ticker}.\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            df = pd.DataFrame.from_dict(data[data_key], orient='index')\n",
        "            if df.empty: return pd.DataFrame()\n",
        "\n",
        "            df = df.reset_index().rename(columns={\n",
        "                'index': 'timestamp', '1. open': 'open', '2. high': 'high',\n",
        "                '3. low': 'low', '4. close': 'close', '5. volume': 'volume'\n",
        "            })\n",
        "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "            for col in ['open', 'high', 'low', 'close', 'volume']:\n",
        "                df[col] = pd.to_numeric(df[col])\n",
        "            df['ticker'] = ticker\n",
        "            try:\n",
        "                df['timestamp'] = df['timestamp'].dt.tz_localize('America/New_York').dt.tz_convert('UTC')\n",
        "            except Exception:\n",
        "                df['timestamp'] = df['timestamp'].dt.tz_localize('UTC') # Fallback\n",
        "            df = df[['timestamp', 'ticker', 'open', 'high', 'low', 'close', 'volume']]\n",
        "            return df.sort_values('timestamp').reset_index(drop=True)\n",
        "        except Exception as e:\n",
        "            print(f\"âœ— Error fetching Alpha Vantage data for {ticker}: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "class NewsAPI:\n",
        "    \"\"\"Handles financial news from NewsAPI\"\"\"\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        print(f\"âœ“ NewsAPI initialized (API key: ...{api_key[-4:]})\")\n",
        "\n",
        "    def get_news(self, ticker: str, company_name: str = None, days_back: int = 7) -> pd.DataFrame:\n",
        "        url = 'https://newsapi.org/v2/everything'\n",
        "        query = company_name if company_name else ticker\n",
        "        from_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
        "        params = {\n",
        "            'q': query, 'from': from_date, 'sortBy': 'publishedAt',\n",
        "            'language': 'en', 'apiKey': self.api_key, 'pageSize': 100\n",
        "        }\n",
        "        try:\n",
        "            response = requests.get(url, params=params, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "            if data['status'] != 'ok':\n",
        "                raise ValueError(f\"API Error: {data.get('message', 'Unknown error')}\")\n",
        "\n",
        "            records = [{\n",
        "                'timestamp': pd.to_datetime(article['publishedAt']).tz_convert('UTC'),\n",
        "                'headline': article['title'],\n",
        "                'description': article.get('description', ''),\n",
        "                'source': article['source']['name'],\n",
        "                'url': article['url'],\n",
        "                'ticker': ticker\n",
        "            } for article in data.get('articles', []) if article.get('title')]\n",
        "\n",
        "            return pd.DataFrame(records).sort_values('timestamp').reset_index(drop=True) if records else pd.DataFrame()\n",
        "        except Exception as e:\n",
        "            print(f\"âœ— Error fetching news for {ticker}: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "class IncrementalDataStorage:\n",
        "    \"\"\"Handles incremental data storage and deduplication.\"\"\"\n",
        "    def __init__(self, data_dir='data/raw'):\n",
        "        self.data_dir = data_dir\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "        self.prices_master_file = os.path.join(data_dir, 'prices_master.csv')\n",
        "        self.news_master_file = os.path.join(data_dir, 'news_master.csv')\n",
        "        self.backup_dir = os.path.join(data_dir, 'daily_backups')\n",
        "        os.makedirs(self.backup_dir, exist_ok=True)\n",
        "\n",
        "    def load_master_data(self) -> Dict[str, pd.DataFrame]:\n",
        "        prices = pd.DataFrame()\n",
        "        news = pd.DataFrame()\n",
        "        if os.path.exists(self.prices_master_file):\n",
        "            prices = pd.read_csv(self.prices_master_file)\n",
        "            prices['timestamp'] = pd.to_datetime(prices['timestamp'])\n",
        "            print(f\"âœ“ Loaded {len(prices):,} existing price records\")\n",
        "        else:\n",
        "            print(\"â„¹ No existing price data found (starting fresh)\")\n",
        "        if os.path.exists(self.news_master_file):\n",
        "            news = pd.read_csv(self.news_master_file)\n",
        "            news['timestamp'] = pd.to_datetime(news['timestamp'])\n",
        "            print(f\"âœ“ Loaded {len(news):,} existing news records\")\n",
        "        else:\n",
        "            print(\"â„¹ No existing news data found (starting fresh)\")\n",
        "        return {'prices': prices, 'news': news}\n",
        "\n",
        "    def append_new_data(self, new_data: Dict[str, pd.DataFrame]):\n",
        "        existing = self.load_master_data()\n",
        "        if not new_data['prices'].empty:\n",
        "            combined_prices = pd.concat([existing['prices'], new_data['prices']], ignore_index=True)\n",
        "            combined_prices = combined_prices.drop_duplicates(subset=['timestamp', 'ticker'], keep='last')\n",
        "            combined_prices = combined_prices.sort_values(['ticker', 'timestamp']).reset_index(drop=True)\n",
        "            combined_prices.to_csv(self.prices_master_file, index=False)\n",
        "            print(f\"âœ“ Saved {len(combined_prices):,} total price records (added {len(new_data['prices'])} new)\")\n",
        "        if not new_data['news'].empty:\n",
        "            combined_news = pd.concat([existing['news'], new_data['news']], ignore_index=True)\n",
        "            combined_news = combined_news.drop_duplicates(subset=['headline', 'ticker'], keep='first')\n",
        "            combined_news = combined_news.sort_values(['ticker', 'timestamp']).reset_index(drop=True)\n",
        "            combined_news.to_csv(self.news_master_file, index=False)\n",
        "            print(f\"âœ“ Saved {len(combined_news):,} total news records (added {len(new_data['news'])} new)\")\n",
        "\n",
        "    def get_statistics(self) -> dict:\n",
        "        data = self.load_master_data()\n",
        "        stats = {'total_price_records': len(data['prices']), 'total_news_articles': len(data['news']), 'tickers': [], 'date_range': None}\n",
        "        if not data['prices'].empty:\n",
        "            stats['tickers'] = data['prices']['ticker'].unique().tolist()\n",
        "            stats['date_range'] = {'start': data['prices']['timestamp'].min(), 'end': data['prices']['timestamp'].max()}\n",
        "            stats['records_per_ticker'] = data['prices'].groupby('ticker').size().to_dict()\n",
        "        return stats\n",
        "\n",
        "class DataCollector:\n",
        "    \"\"\"Main data collection orchestrator\"\"\"\n",
        "    def __init__(self, price_api: StockPriceAPI, news_api: NewsAPI, storage: IncrementalDataStorage):\n",
        "        self.price_api = price_api\n",
        "        self.news_api = news_api\n",
        "        self.storage = storage\n",
        "\n",
        "    def collect_and_store(self, tickers: List[str], interval='5m', period='1d', news_days_back=1):\n",
        "        all_prices, all_news = [], []\n",
        "        company_map = {'AAPL': 'Apple', 'MSFT': 'Microsoft', 'GOOGL': 'Google', 'AMZN': 'Amazon', 'TSLA': 'Tesla'}\n",
        "        for ticker in tickers:\n",
        "            print(f\"\\nðŸ“Š Collecting data for {ticker}...\")\n",
        "            prices = self.price_api.get_intraday_quotes(ticker, interval, period)\n",
        "            if not prices.empty:\n",
        "                all_prices.append(prices)\n",
        "                print(f\"  âœ“ {len(prices)} price records\")\n",
        "            company_name = company_map.get(ticker, ticker)\n",
        "            news = self.news_api.get_news(ticker, company_name, news_days_back)\n",
        "            if not news.empty:\n",
        "                all_news.append(news)\n",
        "                print(f\"  âœ“ {len(news)} news articles\")\n",
        "\n",
        "        new_data = {\n",
        "            'prices': pd.concat(all_prices, ignore_index=True) if all_prices else pd.DataFrame(),\n",
        "            'news': pd.concat(all_news, ignore_index=True) if all_news else pd.DataFrame()\n",
        "        }\n",
        "        self.storage.append_new_data(new_data)\n",
        "        return new_data\n",
        "\n",
        "# === MAIN EXECUTION ===\n",
        "def run_backfill():\n",
        "    TICKERS = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'AMZN']\n",
        "\n",
        "    # Initialize components\n",
        "    price_api = StockPriceAPI(ALPHAVANTAGE_KEY)\n",
        "    news_api = NewsAPI(NEWSAPI_KEY)\n",
        "    storage = IncrementalDataStorage('data/raw')\n",
        "    collector = DataCollector(price_api, news_api, storage)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"  RUNNING ONE-TIME DATA BACKFILL\")\n",
        "    print(\"  This will take > 1 minute...\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    stats_before = storage.get_statistics()\n",
        "    print(f\"  Existing price records: {stats_before['total_price_records']:,}\")\n",
        "    print(f\"  Existing news articles: {stats_before['total_news_articles']:,}\")\n",
        "\n",
        "    new_data = collector.collect_and_store(\n",
        "        tickers=TICKERS,\n",
        "        interval='5m',\n",
        "        # --- THIS IS THE FIX ---\n",
        "        # We set period to '30d'. This is not '1d',\n",
        "        # so our StockPriceAPI class will use 'outputsize=full'\n",
        "        period='30d',\n",
        "        # We also get more news to match the price history\n",
        "        news_days_back=30\n",
        "        # --- END OF FIX ---\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"  âœ… BACKFILL COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    stats_after = storage.get_statistics()\n",
        "    print(f\"  Total price records: {stats_after['total_price_records']:,}\")\n",
        "    print(f\"  Total news articles: {stats_after['total_news_articles']:,}\")\n",
        "    if stats_after.get('date_range'):\n",
        "        print(f\"  New date range: {stats_after['date_range']['start']} to {stats_after['date_range']['end']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_backfill()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKO8KNuEQHgX",
        "outputId": "500d0d3a-9c17-470a-f20f-398fe9d083ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Colab userdata keys loaded.\n",
            "âœ“ Alpha Vantage initialized (API key: ...Z82S)\n",
            "âœ“ NewsAPI initialized (API key: ...d0ea)\n",
            "\n",
            "======================================================================\n",
            "  RUNNING ONE-TIME DATA BACKFILL\n",
            "  This will take > 1 minute...\n",
            "======================================================================\n",
            "âœ“ Loaded 16,996 existing price records\n",
            "âœ“ Loaded 669 existing news records\n",
            "  Existing price records: 16,996\n",
            "  Existing news articles: 669\n",
            "\n",
            "ðŸ“Š Collecting data for AAPL...\n",
            "  ... [Alpha Vantage] Waiting 13s to respect 5 calls/min rate limit...\n",
            "  ... Requesting 'outputsize=full'\n",
            "âœ— [Alpha Vantage] Could not find 'Time Series' data key for AAPL.\n",
            "  âœ“ 96 news articles\n",
            "\n",
            "ðŸ“Š Collecting data for MSFT...\n",
            "  ... [Alpha Vantage] Waiting 13s to respect 5 calls/min rate limit...\n",
            "  ... Requesting 'outputsize=full'\n",
            "âœ— [Alpha Vantage] Could not find 'Time Series' data key for MSFT.\n",
            "  âœ“ 100 news articles\n",
            "\n",
            "ðŸ“Š Collecting data for GOOGL...\n",
            "  ... [Alpha Vantage] Waiting 13s to respect 5 calls/min rate limit...\n",
            "  ... Requesting 'outputsize=full'\n",
            "âœ— [Alpha Vantage] Could not find 'Time Series' data key for GOOGL.\n",
            "  âœ“ 100 news articles\n",
            "\n",
            "ðŸ“Š Collecting data for TSLA...\n",
            "  ... [Alpha Vantage] Waiting 13s to respect 5 calls/min rate limit...\n",
            "  ... Requesting 'outputsize=full'\n",
            "âœ— [Alpha Vantage] Could not find 'Time Series' data key for TSLA.\n",
            "  âœ“ 99 news articles\n",
            "\n",
            "ðŸ“Š Collecting data for AMZN...\n",
            "  ... [Alpha Vantage] Waiting 13s to respect 5 calls/min rate limit...\n",
            "  ... Requesting 'outputsize=full'\n",
            "âœ— [Alpha Vantage] Could not find 'Time Series' data key for AMZN.\n",
            "  âœ“ 97 news articles\n",
            "âœ“ Loaded 16,996 existing price records\n",
            "âœ“ Loaded 669 existing news records\n",
            "âœ“ Saved 732 total news records (added 492 new)\n",
            "\n",
            "======================================================================\n",
            "  âœ… BACKFILL COMPLETE!\n",
            "======================================================================\n",
            "âœ“ Loaded 16,996 existing price records\n",
            "âœ“ Loaded 732 existing news records\n",
            "  Total price records: 16,996\n",
            "  Total news articles: 732\n",
            "  New date range: 2025-10-13 08:00:00+00:00 to 2025-11-12 00:55:00+00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "import torch\n",
        "import sys\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "# --- Configuration ---\n",
        "# --- FIX: Point to the root directory where you uploaded the file ---\n",
        "NEWS_FILE = 'data/raw/news_master.csv'\n",
        "OUTPUT_FILE = 'news_with_finbert_sentiment.csv'\n",
        "# --- End of Fix ---\n",
        "\n",
        "MODEL_NAME = \"ProsusAI/finbert\"\n",
        "\n",
        "# --- Setup (Run this in a Colab cell first) ---\n",
        "# !pip install transformers torch\n",
        "\n",
        "@torch.no_grad() # Disable gradient calculations for speed\n",
        "def get_finbert_sentiment(headlines: list, tokenizer, model) -> list:\n",
        "    \"\"\"\n",
        "    Processes a list of headlines and returns a list of sentiment scores.\n",
        "    \"\"\"\n",
        "    # 1. Tokenize the headlines\n",
        "    inputs = tokenizer(headlines, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "\n",
        "    # 2. Run headlines through the model\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # 3. Get probabilities (softmax)\n",
        "    # The model outputs 3 scores: [positive, negative, neutral]\n",
        "    probabilities = softmax(outputs.logits, dim=1)\n",
        "\n",
        "    # 4. Calculate a single compound score\n",
        "    # We'll use: (positive - negative)\n",
        "    # This gives a score from -1.0 (very negative) to 1.0 (very positive)\n",
        "    positive_probs = probabilities[:, 0]\n",
        "    negative_probs = probabilities[:, 1]\n",
        "    # neutral_probs = probabilities[:, 2] # We don't need this for the compound score\n",
        "\n",
        "    compound_scores = (positive_probs - negative_probs).tolist()\n",
        "\n",
        "    return compound_scores\n",
        "\n",
        "def main():\n",
        "    print(f\"Loading headlines from {NEWS_FILE}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(NEWS_FILE)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"FATAL: {NEWS_FILE} not found. Did you upload it?\")\n",
        "        print(\"Make sure it's in the main directory (not a subfolder).\")\n",
        "        return\n",
        "\n",
        "    # To save time, we only process unique headlines\n",
        "    unique_headlines = df[['headline']].drop_duplicates().reset_index(drop=True)\n",
        "    print(f\"Found {len(unique_headlines)} unique headlines to process.\")\n",
        "\n",
        "    # --- Load FinBERT Model ---\n",
        "    print(f\"Loading FinBERT model ({MODEL_NAME})... (This may take a moment)\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "    print(\"âœ“ Model loaded.\")\n",
        "\n",
        "    # Process in batches for efficiency\n",
        "    batch_size = 32\n",
        "    sentiment_scores = []\n",
        "\n",
        "    start_time = time.time()\n",
        "    for i in range(0, len(unique_headlines), batch_size):\n",
        "        batch_headlines = unique_headlines['headline'][i : i + batch_size].tolist()\n",
        "\n",
        "        print(f\"  Processing batch {i//batch_size + 1}/{len(unique_headlines)//batch_size + 1}...\")\n",
        "        scores = get_finbert_sentiment(batch_headlines, tokenizer, model)\n",
        "        sentiment_scores.extend(scores)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"\\nProcessing complete. Took {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    # Create a DataFrame from the results\n",
        "    sentiment_df = pd.DataFrame({\n",
        "        'headline': unique_headlines['headline'],\n",
        "        'sentiment': sentiment_scores\n",
        "    })\n",
        "\n",
        "    # Merge the new sentiment data back into the original news file\n",
        "    print(\"Merging sentiment data back into main news file...\")\n",
        "    final_df = pd.merge(df, sentiment_df, on='headline', how='left')\n",
        "\n",
        "    # Save the new, enriched file\n",
        "    final_df.to_csv(OUTPUT_FILE, index=False)\n",
        "    print(f\"âœ“ Success! New file saved to {OUTPUT_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # To run in Colab, add this line at the end\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9zeN7oAGxCK",
        "outputId": "9f7b1712-4b6b-4f6f-b767-bb91e81d8d59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading headlines from data/raw/news_master.csv...\n",
            "Found 659 unique headlines to process.\n",
            "Loading FinBERT model (ProsusAI/finbert)... (This may take a moment)\n",
            "âœ“ Model loaded.\n",
            "  Processing batch 1/21...\n",
            "  Processing batch 2/21...\n",
            "  Processing batch 3/21...\n",
            "  Processing batch 4/21...\n",
            "  Processing batch 5/21...\n",
            "  Processing batch 6/21...\n",
            "  Processing batch 7/21...\n",
            "  Processing batch 8/21...\n",
            "  Processing batch 9/21...\n",
            "  Processing batch 10/21...\n",
            "  Processing batch 11/21...\n",
            "  Processing batch 12/21...\n",
            "  Processing batch 13/21...\n",
            "  Processing batch 14/21...\n",
            "  Processing batch 15/21...\n",
            "  Processing batch 16/21...\n",
            "  Processing batch 17/21...\n",
            "  Processing batch 18/21...\n",
            "  Processing batch 19/21...\n",
            "  Processing batch 20/21...\n",
            "  Processing batch 21/21...\n",
            "\n",
            "Processing complete. Took 82.08 seconds.\n",
            "Merging sentiment data back into main news file...\n",
            "âœ“ Success! New file saved to news_with_finbert_sentiment.csv\n"
          ]
        }
      ]
    }
  ]
}